# Score model output predictions

Scores model outputs with a single `output_type` against observed data.

## Usage

``` r
score_model_out(
  model_out_tbl,
  oracle_output,
  metrics = NULL,
  relative_metrics = NULL,
  baseline = NULL,
  summarize = TRUE,
  by = "model_id",
  output_type_id_order = NULL,
  transform = NULL,
  transform_append = FALSE,
  transform_label = NULL,
  ...
)
```

## Arguments

- model_out_tbl:

  Model output tibble with predictions

- oracle_output:

  Predictions that would have been generated by an oracle model that
  knew the observed target data values in advance

- metrics:

  Character vector of scoring metrics to compute. If `NULL` (the
  default), appropriate metrics are chosen automatically. See details
  for more.

- relative_metrics:

  Character vector of scoring metrics for which to compute relative
  skill scores. The `relative_metrics` should be a subset of `metrics`
  and should only include proper scores (e.g., it should not contain
  interval coverage metrics). If `NULL` (the default), no relative
  metrics will be computed. Relative metrics are only computed if
  `summarize = TRUE`, and require that `"model_id"` is included in `by`.

- baseline:

  String with the name of a model to use as a baseline for relative
  skill scores. If a baseline is given, then a scaled relative skill
  with respect to the baseline will be returned. By default (`NULL`),
  relative skill will not be scaled with respect to a baseline model.

- summarize:

  Boolean indicator of whether summaries of forecast scores should be
  computed. Defaults to `TRUE`.

- by:

  Character vector naming columns to summarize by. For example,
  specifying `by = "model_id"` (the default) will compute average scores
  for each model.

- output_type_id_order:

  For ordinal variables in pmf format, this is a vector of levels for
  pmf forecasts, in increasing order of the levels. The order of the
  values for the output_type_id can be found by referencing the hub's
  tasks.json configuration file. For all output types other than pmf,
  this is ignored.

- transform:

  A function to apply as a scale transformation to both predictions and
  observations before scoring. Common choices include
  [`log_shift`](https://epiforecasts.io/scoringutils/reference/log_shift.html)
  (recommended for log transformation as it handles zeros via an offset
  parameter), `sqrt`, or `log1p`. Avoid using `log` directly if data may
  contain zeros. If `NULL` (the default), no transformation is applied.
  Only supported for quantile, mean, and median output types.

- transform_append:

  Logical. If `FALSE` (the default), scores are computed only on the
  transformed scale. If `TRUE`, scores are computed on both original and
  transformed scales, with a `scale` column distinguishing them. Ignored
  if `transform = NULL`.

- transform_label:

  A character string label for the transformation (e.g., "log"). If
  `NULL` (the default), the label is auto-generated from the function
  name (e.g., "log_shift" for
  [`scoringutils::log_shift`](https://epiforecasts.io/scoringutils/reference/log_shift.html)).
  Required when using an anonymous transform function. Ignored if
  `transform = NULL`. Note: the label only appears in output when
  `transform_append = TRUE`, where it distinguishes transformed rows
  (labeled with this value) from original rows (labeled "natural") in
  the `scale` column.

- ...:

  Additional arguments passed to the `transform` function. For example,
  allows use of the `offset` and `base` arguments of
  [`scoringutils::log_shift()`](https://epiforecasts.io/scoringutils/reference/log_shift.html).
  Ignored if `transform = NULL`.

## Value

A data.table with scores

## Details

See the hubverse documentation for the expected format of the [oracle
output
data](https://docs.hubverse.io/en/latest/user-guide/target-data.html#oracle-output).

Default metrics are provided by the `scoringutils` package. You can
select metrics by passing in a character vector of metric names to the
`metrics` argument.

The following metrics can be selected (all are used by default) for the
different `output_type`s:

**Quantile forecasts:** (`output_type == "quantile"`)

- wis

- overprediction

- underprediction

- dispersion

- bias

- ae_median

- "interval_coverage_XX": interval coverage at the "XX" level. For
  example, "interval_coverage_95" is the 95% interval coverage rate,
  which would be calculated based on quantiles at the probability levels
  0.025 and 0.975.

See
[scoringutils::get_metrics.forecast_quantile](https://epiforecasts.io/scoringutils/reference/get_metrics.forecast_quantile.html)
for details.

**Nominal forecasts:** (`output_type == "pmf"` and
`output_type_id_order` is `NULL`)

- log_score

See
[scoringutils::get_metrics.forecast_nominal](https://epiforecasts.io/scoringutils/reference/get_metrics.forecast_nominal.html)
for details.

**Ordinal forecasts:** (`output_type == "pmf"` and
`output_type_id_order` is a vector)

- log_score

- rps

See
[scoringutils::get_metrics.forecast_ordinal](https://epiforecasts.io/scoringutils/reference/get_metrics.forecast_ordinal.html)
for details.

**Median forecasts:** (`output_type == "median"`)

- ae_point: absolute error of the point forecast (recommended for the
  median, see Gneiting (2011))

See
[scoringutils::get_metrics.forecast_point](https://epiforecasts.io/scoringutils/reference/get_metrics.forecast_point.html)
for details.

**Mean forecasts:** (`output_type == "mean"`)

- `se_point`: squared error of the point forecast (recommended for the
  mean, see Gneiting (2011))

See
[scoringutils::add_relative_skill](https://epiforecasts.io/scoringutils/reference/add_relative_skill.html)
for details on relative skill scores.

## References

Gneiting, Tilmann. 2011. "Making and Evaluating Point Forecasts."
Journal of the American Statistical Association 106 (494): 746â€“62.
\<doi: 10.1198/jasa.2011.r10138\>.

## Examples

``` r
# compute WIS and interval coverage rates at 80% and 90% levels based on
# quantile forecasts, summarized by the mean score for each model
quantile_scores <- score_model_out(
  model_out_tbl = hubExamples::forecast_outputs |>
    dplyr::filter(.data[["output_type"]] == "quantile"),
  oracle_output = hubExamples::forecast_oracle_output,
  metrics = c("wis", "interval_coverage_80", "interval_coverage_90"),
  relative_metrics = "wis",
  by = "model_id"
)
quantile_scores
#> Key: <model_id>
#>             model_id      wis interval_coverage_80 interval_coverage_90
#>               <char>    <num>                <num>                <num>
#> 1: Flusight-baseline 329.4545                  0.0               0.1250
#> 2:   MOBS-GLEAM_FLUH 315.2393                  0.5               0.5625
#> 3:          PSI-DICE 227.9527                  0.5               0.5000
#>    wis_relative_skill
#>                 <num>
#> 1:          1.1473659
#> 2:          1.0978597
#> 3:          0.7938733

# compute log scores based on pmf predictions for categorical targets,
# summarized by the mean score for each combination of model and location.
# Note: if the model_out_tbl had forecasts for multiple targets using a
# pmf output_type with different bins, it would be necessary to score the
# predictions for those targets separately.
pmf_scores <- score_model_out(
  model_out_tbl = hubExamples::forecast_outputs |>
    dplyr::filter(.data[["output_type"]] == "pmf"),
  oracle_output = hubExamples::forecast_oracle_output,
  metrics = c("log_score", "rps"),
  by = c("model_id", "location", "horizon"),
  output_type_id_order = c("low", "moderate", "high", "very high")
)
head(pmf_scores)
#>             model_id location horizon   log_score          rps
#>               <char>   <char>   <int>       <num>        <num>
#> 1: Flusight-baseline       25       0  0.02107606 0.0008531043
#> 2: Flusight-baseline       25       1  6.69652380 0.5029240066
#> 3: Flusight-baseline       25       2 17.73313203 1.0057355863
#> 4: Flusight-baseline       25       3         Inf 1.8665126816
#> 5: Flusight-baseline       48       0  2.18418007 0.4873966597
#> 6: Flusight-baseline       48       1  7.49960792 0.9659026096
```
