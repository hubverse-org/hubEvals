% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/score_model_out.R
\name{score_model_out}
\alias{score_model_out}
\title{Score model output predictions with a single \code{output_type} against observed data}
\usage{
score_model_out(
  model_out_tbl,
  target_observations,
  metrics = NULL,
  summarize = TRUE,
  by = "model_id",
  output_type_id_order = NULL
)
}
\arguments{
\item{model_out_tbl}{Model output tibble with predictions}

\item{target_observations}{Observed 'ground truth' data to be compared to
predictions}

\item{metrics}{Optional character vector of scoring metrics to compute. See details for more.}

\item{summarize}{Boolean indicator of whether summaries of forecast scores
should be computed. Defaults to \code{TRUE}.}

\item{by}{Character vector naming columns to summarize by. For example,
specifying \code{by = "model_id"} (the default) will compute average scores for
each model.}

\item{output_type_id_order}{For ordinal variables in pmf format, this is a
vector of levels for pmf forecasts, in increasing order of the levels. For
all other output types, this is ignored.}
}
\value{
forecast_quantile
}
\description{
Score model output predictions with a single \code{output_type} against observed data
}
\details{
If \code{metrics} is \code{NULL} (the default), this function chooses
appropriate metrics based on the \code{output_type} contained in the \code{model_out_tbl}:
\itemize{
\item For \code{output_type == "quantile"}, we use the default metrics provided by
\code{scoringutils::metrics_quantile()}: wis, overprediction, underprediction, dispersion, bias, interval_coverage_50, interval_coverage_90, interval_coverage_deviation, ae_median
\item For \code{output_type == "pmf"} and \code{output_type_id_order} is \code{NULL} (indicating
that the predicted variable is a nominal variable), we use the default metric
provided by \code{scoringutils::metrics_nominal()},
log_score
\item For \code{output_type == "median"}, we use "ae_point"
\item For \code{output_type == "mean"}, we use "se_point"
}

Alternatively, a character vector of scoring metrics can be provided. In this
case, the following options are supported:
\itemize{
\item \code{output_type == "median"} and \code{output_type == "mean"}:
\itemize{
\item "ae_point": absolute error of a point prediction (generally recommended for the median)
\item "se_point": squared error of a point prediction (generally recommended for the mean)
}
\item \code{output_type == "quantile"}:
\itemize{
\item "ae_median": absolute error of the predictive median (i.e., the quantile at probability level 0.5)
\item "wis": weighted interval score (WIS) of a collection of quantile predictions
\item "overprediction": The component of WIS measuring the extent to which
predictions fell above the observation.
\item "underprediction": The component of WIS measuring the extent to which
predictions fell below the observation.
\item "dispersion":  The component of WIS measuring the dispersion of forecast
distributions.
\item "interval_coverage_XX": interval coverage at the "XX" level. For example,
"interval_coverage_95" is the 95\% interval coverage rate, which would be calculated
based on quantiles at the probability levels 0.025 and 0.975.
}
\item \code{output_type == "pmf"}:
\itemize{
\item "log_score": log score
}
}
}
\examples{
\dontshow{if (requireNamespace("hubExamples", quietly = TRUE)) (if (getRversion() >= "3.4") withAutoprint else force)(\{ # examplesIf}
# compute WIS and interval coverage rates at 80\% and 90\% levels based on
# quantile forecasts, summarized by the mean score for each model
quantile_scores <- score_model_out(
  model_out_tbl = hubExamples::forecast_outputs |>
    dplyr::filter(.data[["output_type"]] == "quantile"),
  target_observations = hubExamples::forecast_target_observations,
  metrics = c("wis", "interval_coverage_80", "interval_coverage_90"),
  by = "model_id"
)
quantile_scores

# compute log scores based on pmf predictions for categorical targets,
# summarized by the mean score for each combination of model and location.
# Note: if the model_out_tbl had forecasts for multiple targets using a
# pmf output_type with different bins, it would be necessary to score the
# predictions for those targets separately.
pmf_scores <- score_model_out(
  model_out_tbl = hubExamples::forecast_outputs |>
    dplyr::filter(.data[["output_type"]] == "pmf"),
  target_observations = hubExamples::forecast_target_observations,
  metrics = "log_score",
  by = c("model_id", "location", "horizon")
)
head(pmf_scores)
\dontshow{\}) # examplesIf}
}
